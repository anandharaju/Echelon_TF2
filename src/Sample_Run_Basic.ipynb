{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two-Tier Malware Detection for Raw Executables\n",
    "[ Block-Based Implementation ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisites:\n",
    "\n",
    "* Please refer to the Jupyter notebook - [Data_Preprocessing_and_Partitioning.ipynb](Data_Preprocessing_and_Partitioning.ipynb) for the complete set of one-time pre-requisite steps to be carried for data preprocessing and partitioning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic implementation : Overview:\n",
    "\n",
    "In this implementation, the second tier of our framework leverages the below two information:\n",
    "1. Qualified sections identified by Activation Trend Identification (ATI) mechanism.\n",
    "\n",
    "Unlike, top activation block and section id implementations, this basic implementation uses the entire byte data of\n",
    "a qualified section as training input for Tier-2. During Tier-2 training, the byte data corresponding to \n",
    "unqualified sections are turned-off, i.e., their byte values are set to the padding character (0), and then \n",
    "fed into the Tier-2 model for training. Due to this, the tier-2's input data will be noisy as well as requires a training time same as tier-1 for each qualification criteria cut-off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High-level Sequence of Two-Tier Training-Validation-Testing Process:\n",
    "\n",
    "```bash\n",
    "TRAINING & VALIDATION:\n",
    "----------------------\n",
    "Load Training & Validation Partitions  -> Train & Evaluate Tier-1 model\n",
    "Load Validation Partitions             -> Find Tier-1 model threshold (THD1) + Find & Store B1_Val set into partitions\n",
    "Load Training Partitions again         -> Find & store B1_Train set using THD1\n",
    "Load B1_Train partitions               -> Perform ATI over B1_Train + Find Qualified sections to Train Tier-2\n",
    "Load B1_Train & B1_Val Partitions      -> Find & store top activation blocks into partitions using Qualified sections\n",
    "Load B1 Train & Val Block partitions   -> Train & Evaluate Tier-2 model\n",
    "Load B1_Validation partitions          -> Find Tier-2 model threshold (THD2)\n",
    "\n",
    "TESTING:\n",
    "--------\n",
    "Load Tier-1 Test Partitions            -> Predict using Tier-1 model & store B1_Test set into partitions\n",
    "Load B1_Test partitions                -> Find & store Top activation blocks into B1_Test_Block partitions\n",
    "Load B1_Test_Block partitions          -> Predict using Tier-2 model & reconcile Tier-1 and Tier-2 results\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Run:\n",
    "The below sample run uses 20% of DS1 dataset (approx. 40k samples) and the output is provided for a single fold of cross-validation, where the training is allowed to run for 50 epochs with an early stopping criteria=5. Please note that these sample results are not indicative of actual results.\n",
    "\n",
    "While running new experiments, start with a small early stopping value (param: EARLY_STOPPING_PATIENCE), such as 0 or 1,\n",
    "to check the base time consumption, as each unit increment to this parameter may result in increased training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Platform: linux\n",
      "Using TensorFlow backend.\n",
      "2020-07-05 06:14:18.550090: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6\n",
      "2020-07-05 06:14:18.560155: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6\n",
      "2020-07-05 06:14:30.598968: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
      "2020-07-05 06:14:30.645863: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \n",
      "pciBusID: 0000:04:00.0 name: Tesla P100-PCIE-12GB computeCapability: 6.0\n",
      "coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 11.91GiB deviceMemoryBandwidth: 511.41GiB/s\n",
      "2020-07-05 06:14:30.646016: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2020-07-05 06:14:30.646125: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2020-07-05 06:14:30.656274: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
      "2020-07-05 06:14:30.658838: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
      "2020-07-05 06:14:30.673568: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
      "2020-07-05 06:14:30.677577: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
      "2020-07-05 06:14:30.677691: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2020-07-05 06:14:30.680226: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\n",
      "2020-07-05 06:14:30.680747: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2020-07-05 06:14:30.692412: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2095280000 Hz\n",
      "2020-07-05 06:14:30.693033: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5dd2f40 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-07-05 06:14:30.693088: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2020-07-05 06:14:30.817554: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e59570 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2020-07-05 06:14:30.817640: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-12GB, Compute Capability 6.0\n",
      "2020-07-05 06:14:30.819472: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \n",
      "pciBusID: 0000:04:00.0 name: Tesla P100-PCIE-12GB computeCapability: 6.0\n",
      "coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 11.91GiB deviceMemoryBandwidth: 511.41GiB/s\n",
      "2020-07-05 06:14:30.819596: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2020-07-05 06:14:30.819650: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2020-07-05 06:14:30.819735: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
      "2020-07-05 06:14:30.819804: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
      "2020-07-05 06:14:30.819871: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
      "2020-07-05 06:14:30.819937: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
      "2020-07-05 06:14:30.819990: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2020-07-05 06:14:30.823343: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\n",
      "2020-07-05 06:14:30.823457: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2020-07-05 06:14:31.206184: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2020-07-05 06:14:31.206265: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 \n",
      "2020-07-05 06:14:31.206294: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N \n",
      "2020-07-05 06:14:31.208600: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11288 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-12GB, pci bus id: 0000:04:00.0, compute capability: 6.0)\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "I0705 06:14:31.839179 47348255567552 main.py:15] Tensorflow Version: 2.1.0\n",
      "E0705 06:14:31.840522 47348255567552 main.py:44] GPU related error occurred.\n",
      "Traceback (most recent call last):\n",
      "  File \"main.py\", line 40, in <module>\n",
      "    tf.config.experimental.set_memory_growth(gpu, True)\n",
      "  File \"/project/6027298/aduraira/tf2/lib/python3.6/site-packages/tensorflow_core/python/framework/config.py\", line 494, in set_memory_growth\n",
      "    context.context().set_memory_growth(device, enable)\n",
      "  File \"/project/6027298/aduraira/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/context.py\", line 1241, in set_memory_growth\n",
      "    \"Physical devices cannot be modified after being initialized\")\n",
      "RuntimeError: Physical devices cannot be modified after being initialized\n",
      "I0705 06:14:32.378919 47348255567552 echelon_meta.py:35] #############################################################################################################################\n",
      "I0705 06:14:32.379115 47348255567552 echelon_meta.py:36] \t\t\t\t\t\t\t\t\t RUN SETUP\n",
      "I0705 06:14:32.379225 47348255567552 echelon_meta.py:37] #############################################################################################################################\n",
      "\n",
      "I0705 06:14:32.379319 47348255567552 echelon_meta.py:38] Project Base                              : /home/aduraira/projects/def-wangk/aduraira/Echelon_TF2\n",
      "I0705 06:14:32.379414 47348255567552 echelon_meta.py:39] Raw & Pickle Data source                  : /home/aduraira/projects/def-wangk/aduraira/partitions/ds1_pkl_100_percent/\n",
      "I0705 06:14:32.379498 47348255567552 echelon_meta.py:40] CSV path for the dataset                  : /home/aduraira/projects/def-wangk/aduraira/Echelon_TF2/data/ds1_pkl.csv\n",
      "I0705 06:14:32.379578 47348255567552 echelon_meta.py:41] GPU Enabled                               : True\n",
      "I0705 06:14:32.379658 47348255567552 echelon_meta.py:42] Folds                                     : 5\n",
      "I0705 06:14:32.380517 47348255567552 echelon_meta.py:44] Batch Size [Train T1 : Train T2 : Predict]: [ 64 : 64 : 64 ]\n",
      "I0705 06:14:32.380600 47348255567552 echelon_meta.py:45] Epochs [Tier1 : Tier2]                    : [ 1 : 1 ]\n",
      "I0705 06:14:32.380673 47348255567552 echelon_meta.py:46] \n",
      "\n",
      "I0705 06:14:32.380750 47348255567552 echelon_meta.py:47] Model used for Tier-1 Training            : Pre-trained Malconv - No exposure to Echelon training data\n",
      "I0705 06:14:32.380821 47348255567552 echelon_meta.py:48] Model used for Tier-2 Training            : Pre-trained Malconv - No exposure to Echelon section-wise training data\n",
      "I0705 06:14:32.380903 47348255567552 echelon_meta.py:49] Target FPR [ Overall : Tier1 : Tier2 ]    : [ 0.1 : 0.1 : 0 ]\n",
      "I0705 06:14:32.380975 47348255567552 echelon_meta.py:50] Skip Tier-1 Training                      : False\n",
      "I0705 06:14:32.381056 47348255567552 echelon_meta.py:51] Skip Tier-2 Training                      : False\n",
      "I0705 06:14:32.381134 47348255567552 echelon_meta.py:52] Skip ATI Processing                       : False\n",
      "I0705 06:14:32.381211 47348255567552 echelon_meta.py:53] Boost benign files with a lower bound     : True\n",
      "I0705 06:14:32.381290 47348255567552 echelon_meta.py:54] Percentiles for Q_Criteria selection      : [85, 88, 90, 92]\n",
      "I0705 06:14:32.382109 47348255567552 echelon_meta.py:56] Maximum file size set for the model input : 1048576bytes\n",
      "I0705 06:14:32.382194 47348255567552 echelon_meta.py:57] Maximum size set for Section Byte Map     : 2000\n",
      "I0705 06:14:32.382275 47348255567552 echelon_meta.py:58] Layer Number to stunt plugin model for ATI: 4\n",
      "I0705 06:14:32.382363 47348255567552 echelon_meta.py:59] CNN Window size | Stride size             : 500|500\n",
      "I0705 06:14:32.382484 47348255567552 generate_train_predict.py:99] \n",
      "START TIME  [ 05/07/2020 06:14:32 ]\n",
      "I0705 06:14:32.435413 47348255567552 generate_train_predict.py:112] Total Partition: 7\n",
      "I0705 06:14:32.437050 47348255567552 generate_train_predict.py:119] Train: 4   Val: 1   Test: 2\n",
      "I0705 06:14:32.469883 47348255567552 generate_train_predict.py:129] >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> [ CV-FOLD 1/5 ]     Training: 4    Validation: 1    Testing: 2\n",
      "I0705 06:14:32.470104 47348255567552 generate_train_predict.py:130] Partition List: train [1, 2, 3, 4]   val [0]   test [5, 6]\n",
      "I0705 06:14:32.494380 47348255567552 generate_train_predict.py:129] >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> [ CV-FOLD 2/5 ]     Training: 4    Validation: 1    Testing: 2\n",
      "I0705 06:14:32.494570 47348255567552 generate_train_predict.py:130] Partition List: train [0, 2, 3, 4]   val [1]   test [5, 6]\n",
      "I0705 06:14:32.494909 47348255567552 train.py:357] ************************ TIER 1 TRAINING - STARTED ****************************\n",
      "I0705 06:14:32.500700 47348255567552 train.py:137] [ CAUTION ] : Resuming with pretrained model for TIER1 - ember_malconv.h5\n",
      "I0705 06:14:33.162265 47348255567552 train.py:372] [ PARTITION LEVEL TIER-1 EPOCH  : 1 ]\n",
      "I0705 06:14:33.162534 47348255567552 train.py:376] Training on partition: 0\n",
      "2020-07-05 06:15:08.632469: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2020-07-05 06:15:09.139215: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2020-07-05 06:15:10.079070: W tensorflow/stream_executor/gpu/redzone_allocator.cc:312] Not found: ./bin/ptxas not found\n",
      "Relying on driver to perform ptx compilation. This message will be only logged once.\n",
      "I0705 06:18:00.718497 47348255567552 train.py:376] Training on partition: 2\n",
      "I0705 06:21:25.940566 47348255567552 train.py:376] Training on partition: 3\n",
      "I0705 06:24:45.987231 47348255567552 train.py:376] Training on partition: 4\n",
      "I0705 06:28:05.475897 47348255567552 train.py:392] Evaluating on validation data . . .\n",
      "I0705 06:30:44.139884 47348255567552 train.py:415] Current Epoch Loss: 0.10965821146965027      \tCurrent Epoch Acc: 0.9829047322273254       \tUpdating best loss: 0.10965821146965027\n",
      "I0705 06:30:44.143766 47348255567552 train.py:372] [ PARTITION LEVEL TIER-1 EPOCH  : 2 ]\n",
      "I0705 06:30:44.158652 47348255567552 train.py:376] Training on partition: 0\n",
      "I0705 06:34:25.331647 47348255567552 train.py:376] Training on partition: 2\n",
      "I0705 06:38:07.185272 47348255567552 train.py:376] Training on partition: 3\n",
      "I0705 06:41:42.203122 47348255567552 train.py:376] Training on partition: 4\n",
      "I0705 06:45:15.858954 47348255567552 train.py:392] Evaluating on validation data . . .\n",
      "I0705 06:47:52.952306 47348255567552 train.py:415] Current Epoch Loss: 0.0010564138647168875    \tCurrent Epoch Acc: 0.9863525629043579       \tUpdating best loss: 0.0010564138647168875\n",
      "I0705 06:47:52.955690 47348255567552 train.py:372] [ PARTITION LEVEL TIER-1 EPOCH  : 3 ]\n",
      "I0705 06:47:52.962584 47348255567552 train.py:376] Training on partition: 0\n",
      "I0705 06:51:33.869347 47348255567552 train.py:376] Training on partition: 2\n",
      "I0705 06:55:16.570130 47348255567552 train.py:376] Training on partition: 3\n",
      "I0705 06:58:50.922764 47348255567552 train.py:376] Training on partition: 4\n",
      "I0705 07:02:10.798300 47348255567552 train.py:392] Evaluating on validation data . . .\n",
      "I0705 07:04:49.244600 47348255567552 train.py:417] Current Epoch Loss: 0.05454102158546448\tCurrent Epoch Acc: 0.9880764484405518\n",
      "I0705 07:04:49.244910 47348255567552 train.py:419] 1 epochs passed since best val loss of 0.0010564138647168875\n",
      "I0705 07:04:49.251688 47348255567552 train.py:421] Triggering early stopping as no improvement found since last 1 epochs!  Best Loss: 0.0010564138647168875\n",
      "I0705 07:04:58.786272 47348255567552 train.py:435] ************************ TIER 1 TRAINING - ENDED ****************************\n",
      "I0705 07:04:58.786476 47348255567552 train.py:449] *** Prediction over Validation data in TIER-1 to select THD1 and Boosting Bound\n",
      "I0705 07:07:40.331238 47348255567552 predict.py:144] Selected Threshold: 99.6896225203675    TPR: 91.112\tFPR:  0.087\n",
      "I0705 07:07:40.367332 47348255567552 predict.py:166] Setting B2 boosting threshold: 1.1916155e-05\n",
      "I0705 07:07:40.461693 47348255567552 predict.py:182] Number of files boosted to B2=1466 \t[ 3770 - 2304 ]     Boosting Bound used: 1.1916155e-05   Escaped FNs:0\n",
      "I0705 07:07:42.161833 47348255567552 collect_exe_files.py:18] Total number of files to partition: 2304\n",
      "I0705 07:12:02.232437 47348255567552 collect_exe_files.py:75] Created Partition b1_val_1_p0 with 2304 files and tracker csv with 2304 files.\n",
      "I0705 07:12:03.536508 47348255567552 train.py:479] *** Prediction over Training data in TIER-1 to generate B1 data for TIER-2 Training\n",
      "I0705 07:14:45.272837 47348255567552 predict.py:182] Number of files boosted to B2=1449 \t[ 3714 - 2265 ]     Boosting Bound used: 1.1916155017388519e-05   Escaped FNs:0\n",
      "I0705 07:17:29.233479 47348255567552 predict.py:182] Number of files boosted to B2=1567 \t[ 3813 - 2246 ]     Boosting Bound used: 1.1916155017388519e-05   Escaped FNs:0\n",
      "I0705 07:20:13.656145 47348255567552 predict.py:182] Number of files boosted to B2=1486 \t[ 3670 - 2184 ]     Boosting Bound used: 1.1916155017388519e-05   Escaped FNs:0\n",
      "I0705 07:22:57.719096 47348255567552 predict.py:182] Number of files boosted to B2=1417 \t[ 3688 - 2271 ]     Boosting Bound used: 1.1916155017388519e-05   Escaped FNs:0\n",
      "I0705 07:22:59.313284 47348255567552 collect_exe_files.py:18] Total number of files to partition: 8966\n",
      "I0705 07:36:29.394361 47348255567552 collect_exe_files.py:46] Created Partition b1_train_1_p0 with 7165 files and tracker csv with 7165 files.\n",
      "I0705 07:39:48.461530 47348255567552 collect_exe_files.py:75] Created Partition b1_train_1_p1 with 1801 files and tracker csv with 1801 files.\n",
      "I0705 07:39:49.506071 47348255567552 train.py:503] Loading stored B1 Data from Training set to train Tier-2 model\n",
      "I0705 07:39:49.549788 47348255567552 train.py:515] ATI - PROCESSING BENIGN AND MALWARE FILES\t\t\tB1 FILES COUNT: 8966 [# Partitions: 2 ]\n",
      "I0705 07:39:49.550057 47348255567552 train.py:516] -----------------------------------------\n",
      "I0705 07:39:51.298589 47348255567552 activation_trend_identification.py:421] ATI for partition: 0\n",
      "I0705 07:41:17.693760 47348255567552 activation_trend_identification.py:239] FMAP MODULE Total B1 [7165]\tGroundTruth [6439:726]\n",
      "I0705 07:44:16.881926 47348255567552 activation_trend_identification.py:421] ATI for partition: 1\n",
      "I0705 07:44:34.879682 47348255567552 activation_trend_identification.py:239] FMAP MODULE Total B1 [1801]\tGroundTruth [1618:183]\n",
      "I0705 07:45:15.822375 47348255567552 activation_trend_identification.py:69] Qsections found - 4\n",
      "I0705 07:45:15.822594 47348255567552 activation_trend_identification.py:70] dict_keys([-0.003844501584874549, 0.04268975793539847, 0.27939945292808804, 0.46825300069889286])\n",
      "I0705 07:45:15.861710 47348255567552 train.py:531] ************************ TIER 2 TRAINING - STARTED ****************************       # Samples: 8966\n",
      "I0705 07:45:15.861829 47348255567552 train.py:542] Updated Tier-2 target FPR: 0\n",
      "I0705 07:45:15.964372 47348255567552 train.py:175] [ CAUTION ] : Resuming with pretrained model for TIER2 - ember_malconv.h5\n",
      "I0705 07:45:16.183962 47348255567552 train.py:560] [-0.003844501584874549, 0.04268975793539847, 0.27939945292808804, 0.4682530006988929]\n",
      "Checking Q_Criterion: -0.003844501584874549 \n",
      " ['.00cfg' '.CRT' '.cdata' '.data' '.data_cy' '.edata' '.gfids' '.header'\n",
      " '.idata' '.itext' '.pdata' '.rdata' '.reloc' '.rodata' '.rsrc' '.sdata'\n",
      " '.stabstr' '.text' '.tls' '.xdata' '/14' '/19' '/29' '/4' '/41' '/55'\n",
      " '/67' 'CODE' 'DATA' 'INIT' 'OVERLAY' 'PAGE' 'PAGELK' 'UPX0' 'UPX1'\n",
      " 'dktlimc' 'ecbtwma']\n",
      "I0705 07:45:16.184433 47348255567552 train.py:563] Tier-2 CNN Training over B1_Train set [Total # Partitions: 2]\n",
      "I0705 07:45:16.184528 47348255567552 train.py:572] [ PARTITIONS LEVEL TIER-2 EPOCH : 1 ]\n",
      "I0705 07:45:16.184619 47348255567552 train.py:576] Training on partition: 0\n",
      "I0705 07:54:01.850250 47348255567552 train.py:576] Training on partition: 1\n",
      "I0705 07:56:19.800117 47348255567552 train.py:594] Evaluating on B1 validation data. . .\n",
      "I0705 07:59:00.237420 47348255567552 train.py:620] Current Tier-2 Epoch Loss: 0.22330588102340698      \t Epoch Acc: 0.9075892567634583       \tUpdating best loss: 0.22330588102340698\n",
      "I0705 07:59:00.237747 47348255567552 train.py:572] [ PARTITIONS LEVEL TIER-2 EPOCH : 2 ]\n",
      "I0705 07:59:00.244328 47348255567552 train.py:576] Training on partition: 0\n",
      "I0705 08:08:01.639695 47348255567552 train.py:576] Training on partition: 1\n",
      "I0705 08:10:22.787531 47348255567552 train.py:594] Evaluating on B1 validation data. . .\n",
      "I0705 08:13:06.027133 47348255567552 train.py:620] Current Tier-2 Epoch Loss: 0.10698254406452179      \t Epoch Acc: 0.9473214149475098       \tUpdating best loss: 0.10698254406452179\n",
      "I0705 08:13:06.031768 47348255567552 train.py:572] [ PARTITIONS LEVEL TIER-2 EPOCH : 3 ]\n",
      "I0705 08:13:06.032047 47348255567552 train.py:576] Training on partition: 0\n",
      "I0705 08:22:12.406641 47348255567552 train.py:576] Training on partition: 1\n",
      "I0705 08:24:33.577867 47348255567552 train.py:594] Evaluating on B1 validation data. . .\n",
      "I0705 08:27:16.570008 47348255567552 train.py:622] Current Tier-2 Epoch Loss: 0.27239754796028137      \t Current Tier-2 Epoch Acc: 0.9459821581840515       \n",
      "I0705 08:27:16.570237 47348255567552 train.py:624] 1 epochs passed since best val loss of 0.10698254406452179\n",
      "I0705 08:27:16.570330 47348255567552 train.py:626] Tier-2 Triggering early stopping as no improvement found since last 1 epochs!    Best Loss:\n",
      "I0705 08:27:22.865974 47348255567552 train.py:641] ************************ TIER 2 TRAINING - ENDED ****************************\n",
      "I0705 08:27:22.866194 47348255567552 train.py:650] Tier-2 Validation over B1_Val set [# Partitions: 1]\n",
      "I0705 08:27:22.866292 47348255567552 train.py:652] Validating partition: 0\n",
      "I0705 08:30:08.255232 47348255567552 predict.py:144] Selected Threshold: 92.8591132164002    TPR: 15.434\tFPR:  0.050\n",
      "I0705 08:30:13.963494 47348255567552 train.py:694] Best Q-criterion so far . . . -0.003844501584874549\t\t92.8591132164002\t0.050175614651279475\t15.434083601286176\n",
      "I0705 08:30:13.966675 47348255567552 train.py:181] [ CAUTION ] : Resuming with old model\n",
      "I0705 08:30:14.041016 47348255567552 train.py:560] [-0.003844501584874549, 0.04268975793539847, 0.27939945292808804, 0.4682530006988929]\n",
      "Checking Q_Criterion: 0.04268975793539847 \n",
      " ['.00cfg' '.CRT' '.cdata' '.data' '.edata' '.gfids' '.header' '.idata'\n",
      " '.itext' '.pdata' '.rdata' '.reloc' '.rodata' '.rsrc' '.sdata' '.text'\n",
      " '.tls' '/14' '/19' '/29' '/4' '/41' '/55' '/67' 'CODE' 'DATA' 'INIT'\n",
      " 'OVERLAY' 'PAGE' 'PAGELK' 'UPX1']\n",
      "I0705 08:30:14.041383 47348255567552 train.py:563] Tier-2 CNN Training over B1_Train set [Total # Partitions: 2]\n",
      "I0705 08:30:14.041515 47348255567552 train.py:572] [ PARTITIONS LEVEL TIER-2 EPOCH : 1 ]\n",
      "I0705 08:30:14.041645 47348255567552 train.py:576] Training on partition: 0\n",
      "I0705 08:39:00.421845 47348255567552 train.py:576] Training on partition: 1\n",
      "I0705 08:41:15.876241 47348255567552 train.py:594] Evaluating on B1 validation data. . .\n",
      "I0705 08:43:52.617882 47348255567552 train.py:620] Current Tier-2 Epoch Loss: 0.16029414534568787      \t Epoch Acc: 0.9482142925262451       \tUpdating best loss: 0.16029414534568787\n",
      "I0705 08:43:52.620126 47348255567552 train.py:572] [ PARTITIONS LEVEL TIER-2 EPOCH : 2 ]\n",
      "I0705 08:43:52.620438 47348255567552 train.py:576] Training on partition: 0\n",
      "I0705 08:52:42.315205 47348255567552 train.py:576] Training on partition: 1\n",
      "I0705 08:54:59.572539 47348255567552 train.py:594] Evaluating on B1 validation data. . .\n"
     ]
    }
   ],
   "source": [
    "!python main.py 2  # Flag ONLY_TIER1_TRAINING is set to True "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tier-1 Results:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image \n",
    "import os\n",
    "img_path = '../out/imgs/'\n",
    "for _, _, imgs in os.walk(img_path):\n",
    "    imgs.sort()\n",
    "    for img in imgs:\n",
    "        if 'Tier1' in img:\n",
    "            print(img[:-4])\n",
    "            pil_img = Image(filename=os.path.join(img_path, img), width = 600, height = 400)\n",
    "            display(pil_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image \n",
    "import os\n",
    "img_path = '../out/imgs/'\n",
    "for _, _, imgs in os.walk(img_path):\n",
    "    imgs.sort()\n",
    "    for img in imgs:\n",
    "        if 'Tier2' in img:\n",
    "            print(img[:-4])\n",
    "            pil_img = Image(filename=os.path.join(img_path, img), width = 600, height = 400)\n",
    "            display(pil_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image \n",
    "import os\n",
    "img_path = '../out/imgs/'\n",
    "for _, _, imgs in os.walk(img_path):\n",
    "    imgs.sort()\n",
    "    for img in imgs:\n",
    "        if 'auc' in img:\n",
    "            print(img[:-4])\n",
    "            pil_img = Image(filename=os.path.join(img_path, img), width = 600, height = 400)\n",
    "            display(pil_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
