{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two-Tier Architecture - Data Pre-processing & Partitioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing:\n",
    "   This notebook serves to initiate the one-time data-processing pipeline of the 2-tier architecture. The activities involved here are:\n",
    "* Generating the image-like representation of byte sequence data for whole PE sample as well as PE section level byte sequences, which are stored in separate pickle files for efficiency purposes.\n",
    "\n",
    "* Collecting the list of all PE sections available in the given dataset ```available_sections.csv``` and generating their corresponding word-embeddings using Facebook's 'fasttext' library. The section name to word embedding mappings are stored as a lookup file which is used during Tier-2 operations - ```section_embeddings.csv```.\n",
    "Module Name: src/prepare_dataset.py\n",
    "\n",
    "### Pre-requisites:\n",
    "1. Raw PE samples are placed in required (raw_pe) directory.\n",
    "2. Virtual environment and ipykernel setup as per requirements file.\n",
    "3. 2x additional storage space in the disk to store pre-processed data, where 'x' is size of the supplied dataset.\n",
    "4. Supplied samples should be parse-able by 'pefile' python library.\n",
    "5. Setup folder paths in ```settings.py```\n",
    "\n",
    "**Input:**\n",
    "* List of directory paths (under raw_pe) that contain Benign and Malware raw PE samples.\n",
    "* Boolean flag to indicate whether samples present in each directory are Benign or Malware.\n",
    "\n",
    "**Outcome:**\n",
    "* Raw_To_Pickle.csv: Contains the following fields to aid mapping of a raw sample to a pickle file and vice-versa:\n",
    "\n",
    "|Indexed_File_Name|Benign-0 / Malware-1|Original_File_Name|MD5|SHA1|SHA256|\n",
    "|-----------------|--------------------|----------------|---|----|------|\n",
    "|pe_<PICKLE_FILE_INDEX>.pkl|1|xxx.exe|sample_md5|sample_sha1|sample_sha256|\n",
    "\n",
    "\n",
    "* Structure of files generated:\n",
    "``` bash\n",
    "    Each Indexed_File_Name points to:\n",
    "                |\n",
    "                └── PKL_SOURCE_PATH                           \n",
    "                        └── t1\n",
    "                            └── pe_<PICKLE_FILE_INDEX>.pkl\n",
    "                        └── t2\n",
    "                            └── pe_<PICKLE_FILE_INDEX>.pkl \n",
    "```\n",
    "* Format of Tier-1 pickle file:\n",
    "```bash\n",
    "    {\n",
    "        \"whole_bytes\"        : < IMAGE REPRESENTATION FOR WHOLE BYTE SEQUENCE >, \n",
    "        \"benign\"             : < IS BENIGN ? >\n",
    "    }\n",
    "```\n",
    "* Format of Tier-2 pickle file:\n",
    "```bash\n",
    "    {\n",
    "        \"benign\"           : < IS BENIGN ? >,\n",
    "        \"size_byte\"        : <>,           \n",
    "        \"section_info\"     : {\n",
    "                                 \"SECTION_NAME\" : {\n",
    "                                                      \"section_data\"      : <>,\n",
    "                                                      \"section_size_byte\" : <>,\n",
    "                                                      \"section_bounds\"    : {\n",
    "                                                                                \"start_offset\" : <>,\n",
    "                                                                                \"end_offset\"   : <>\n",
    "    }}}}\n",
    "```\n",
    "\n",
    "*__Note:__* Any information that violates the confidentiality, is not retained in the final set of pickle files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Platform: win32\n",
      "No fold index passed through CLI. Running all folds\n",
      "Total Count: 1 Unprocessed/Skipped: 0\n",
      "Total Count: 2 Unprocessed/Skipped: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"../prepare_dataset.py\", line 130, in <module>\n",
      "    total_unprocessed, total_processed = raw_pe_to_pkl(dir, cnst.RAW_SAMPLE_DIRS[dir], total_unprocessed, total_processed)\n",
      "  File \"../prepare_dataset.py\", line 110, in raw_pe_to_pkl\n",
      "    pd.DataFrame(list_idx).to_csv(cnst.DATASET_BACKUP_FILE, index=False, header=None, mode='a')\n",
      "  File \"C:\\Users\\anand\\Anaconda3\\envs\\tf2\\lib\\site-packages\\pandas\\core\\generic.py\", line 3204, in to_csv\n",
      "    formatter.save()\n",
      "  File \"C:\\Users\\anand\\Anaconda3\\envs\\tf2\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\", line 188, in save\n",
      "    compression=dict(self.compression_args, method=self.compression),\n",
      "  File \"C:\\Users\\anand\\Anaconda3\\envs\\tf2\\lib\\site-packages\\pandas\\io\\common.py\", line 428, in get_handle\n",
      "    f = open(path_or_buf, mode, encoding=encoding, newline=\"\")\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'D:\\\\03_GitWorks\\\\src\\\\data\\\\Raw_To_Pickle.csv'\n"
     ]
    }
   ],
   "source": [
    "!python ../prepare_dataset.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Partitioning\n",
    "\n",
    "__Why Partitioning?__\n",
    "\n",
    "The outcome of data pre-processing step above is a set of pickle files for Tier-1&2. Owing to their collectively huge size, they cannot be kept entirely in memory for processing. Also, it incurs huge disk-read overhead, when we store them as separate pickle files in disk and access them individually during runtime ad-hoc. \n",
    "\n",
    "To reduce disk-reads as well as to fully utilize available memory, we implement partitioning of the pickle files.\n",
    "\n",
    "```bash\n",
    "        .\n",
    "        ├── partitions                             \n",
    "           └── <Specify_Dir_Name>\n",
    "               ├── master_partition_tracker.csv\n",
    "               ├── p<partition_index>.csv\n",
    "               ├── t1_p<partition_index>.pkl\n",
    "               └── t2_p<partition_index>.pkl\n",
    "```\n",
    "\n",
    "__What we call as 'Partition'?__\n",
    "\n",
    "The grouping of several pickle files' data into a single large pickle file is referred to as 'Partition' here and it follows the JSON format below for both Tier-1 and Tier-2 partitions:\n",
    "\n",
    "```bash\n",
    "<t1|t2>_p<partition_index>.pkl\n",
    "    {\n",
    "        key=pe_<PICKLE_FILE_INDEX 1>: value={<TIER_1_DATA | TIER_2_DATA>}\n",
    "        key=pe_<PICKLE_FILE_INDEX 2>: value={<TIER_1_DATA | TIER_2_DATA>}\n",
    "        . \n",
    "        . \n",
    "        key=pe_<PICKLE_FILE_INDEX n>: value={<TIER_1_DATA | TIER_2_DATA>}\n",
    "    }\n",
    "```\n",
    "\n",
    "The data partitions are generated through a stratified sampling process using the list of DS1 samples, such that each partition contains fairly equal ratio of benign and malware. The size of the partitions can be controlled either by required partition size or by allowed number of files per partition, using below configuration parameters in ```settings.py```.\n",
    "\n",
    "```bash\n",
    "        PARTITION_BY_COUNT        # Set to True for partitioning by file count. Otherwise, partition by allowed size.\n",
    "        MAX_PARTITION_SIZE        # Use value equivalent 2GB in bytes\n",
    "        MAX_FILES_PER_PARTITION   # Ex: 7000\n",
    "```\n",
    "\n",
    "__Guideline:__\n",
    "Assuming an available memory of 128GB & usage of batch_size between 64 and 128, We typically use partitions of size 2GB that can hold pickle data for approximately 7000 samples.\n",
    "\n",
    "\n",
    "**Input:**\n",
    "* Accepts the \"Raw_To_Pickle.csv\" generated at the end of pre-processing step as input.\n",
    "* Set the follwing flags with provided values in src\\config\\settings.py\n",
    "\n",
    "```bash\n",
    "        REGENERATE_DATA = True        # To stratify input files list\n",
    "        REGENERATE_PARTITIONS = True\n",
    "        SKIP_CROSS_VALIDATION = True  # To perform pnly partitioning\n",
    "```\n",
    "\n",
    "**Outcomes:**\n",
    "* master_partition_tracker.csv: Holds the total number of partitions generated.\n",
    "* A directory called \"partitions\" outside project directory containing the actual partitioned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Platform: win32"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-03 13:07:25.344868: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found\n",
      "2020-07-03 13:07:25.345543: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2020-07-03 13:07:32.432618: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\n",
      "2020-07-03 13:07:33.225297: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: GeForce GTX 1050 computeCapability: 6.1\n",
      "coreClock: 1.493GHz coreCount: 5 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 104.43GiB/s\n",
      "2020-07-03 13:07:33.230084: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found\n",
      "2020-07-03 13:07:33.234403: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cublas64_10.dll'; dlerror: cublas64_10.dll not found\n",
      "2020-07-03 13:07:33.238664: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cufft64_10.dll'; dlerror: cufft64_10.dll not found\n",
      "2020-07-03 13:07:33.243263: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'curand64_10.dll'; dlerror: curand64_10.dll not found\n",
      "2020-07-03 13:07:33.247673: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cusolver64_10.dll'; dlerror: cusolver64_10.dll not found\n",
      "2020-07-03 13:07:33.251973: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cusparse64_10.dll'; dlerror: cusparse64_10.dll not found\n",
      "2020-07-03 13:07:33.268517: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\n",
      "2020-07-03 13:07:33.269122: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1598] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2020-07-03 13:07:33.271010: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No fold number passed through CLI. Running all folds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2020-07-03 13:07:33.287876: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2cb3a0477f0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-07-03 13:07:33.288620: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2020-07-03 13:07:33.289584: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2020-07-03 13:07:33.290290: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      \n",
      "2020-07-03 13:07:33.302476: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: GeForce GTX 1050 computeCapability: 6.1\n",
      "coreClock: 1.493GHz coreCount: 5 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 104.43GiB/s\n",
      "2020-07-03 13:07:33.310344: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found\n",
      "2020-07-03 13:07:33.315821: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cublas64_10.dll'; dlerror: cublas64_10.dll not found\n",
      "2020-07-03 13:07:33.320922: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cufft64_10.dll'; dlerror: cufft64_10.dll not found\n",
      "2020-07-03 13:07:33.326472: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'curand64_10.dll'; dlerror: curand64_10.dll not found\n",
      "2020-07-03 13:07:33.331143: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cusolver64_10.dll'; dlerror: cusolver64_10.dll not found\n",
      "2020-07-03 13:07:33.335836: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cusparse64_10.dll'; dlerror: cusparse64_10.dll not found\n",
      "2020-07-03 13:07:33.336617: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\n",
      "2020-07-03 13:07:33.337104: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1598] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2020-07-03 13:07:33.464764: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2020-07-03 13:07:33.465352: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \n",
      "2020-07-03 13:07:33.465746: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \n",
      "2020-07-03 13:07:33.471955: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2cb3a229640 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2020-07-03 13:07:33.472686: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1050, Compute Capability 6.1\n",
      "Using TensorFlow backend.\n",
      "2020-07-03 13:07:33,474 :: WARNING :: Mixed precision compatibility check (mixed_float16): WARNING\n",
      "The dtype policy mixed_float16 may run slowly because this machine does not have a GPU.\n",
      "If you will use compatible GPU(s) not attached to this host, e.g. by running a multi-worker model, you can ignore this warning. This message will only be logged once\n",
      "2020-07-03 13:07:33,474 :: INFO :: Tensorflow Version: 2.2.0\n",
      "2020-07-03 13:07:33,599 :: INFO :: #############################################################################################################################\n",
      "2020-07-03 13:07:33,599 :: INFO :: \t\t\t\t\t\t\t\t\t RUN SETUP\n",
      "2020-07-03 13:07:33,600 :: INFO :: #############################################################################################################################\n",
      "\n",
      "2020-07-03 13:07:33,600 :: INFO :: Project Base                              : D:\\03_GitWorks\\Block\n",
      "2020-07-03 13:07:33,600 :: INFO :: Raw & Pickle Data source                  : D:\\08_Dataset\\Internal\\mar2020\\partitions\\xs_partition\\\n",
      "2020-07-03 13:07:33,600 :: INFO :: CSV path for the dataset                  : D:\\03_GitWorks\\Block\\data\\ds1_pkl.csv\n",
      "2020-07-03 13:07:33,600 :: INFO :: GPU Enabled                               : True\n",
      "2020-07-03 13:07:33,600 :: INFO :: Folds                                     : 5\n",
      "2020-07-03 13:07:33,603 :: INFO :: Batch Size [Train T1 : Train T2 : Predict]: [ 32 : 128 : 32 ]\n",
      "2020-07-03 13:07:33,603 :: INFO :: Epochs [Tier1 : Tier2]                    : [ 1 : 1 ]\n",
      "2020-07-03 13:07:33,603 :: INFO :: \n",
      "\n",
      "2020-07-03 13:07:33,603 :: INFO :: Model used for Tier-1 Training            : Pre-trained Malconv - No exposure to Echelon training data\n",
      "2020-07-03 13:07:33,603 :: INFO :: Model used for Tier-2 Training            : Pre-trained Malconv - No exposure to Echelon section-wise training data\n",
      "2020-07-03 13:07:33,604 :: INFO :: Target FPR [ Overall : Tier1 : Tier2 ]    : [ 0.1 : 0.1 : 0 ]\n",
      "2020-07-03 13:07:33,604 :: INFO :: Skip Tier-1 Training                      : False\n",
      "2020-07-03 13:07:33,604 :: INFO :: Skip Tier-2 Training                      : False\n",
      "2020-07-03 13:07:33,604 :: INFO :: Skip ATI Processing                       : False\n",
      "2020-07-03 13:07:33,604 :: INFO :: Boost benign files with a lower bound     : True\n",
      "2020-07-03 13:07:33,605 :: INFO :: Percentiles for Q_Criteria selection      : [90, 92, 94]\n",
      "2020-07-03 13:07:33,606 :: INFO :: Maximum file size set for the model input : 1048576bytes\n",
      "2020-07-03 13:07:33,606 :: INFO :: Maximum size set for Section Byte Map     : 2000\n",
      "2020-07-03 13:07:33,606 :: INFO :: Layer Number to stunt plugin model for ATI: 4\n",
      "2020-07-03 13:07:33,606 :: INFO :: CNN Window size | Stride size             : 500|500\n",
      "2020-07-03 13:07:33,607 :: INFO :: \n",
      "START TIME  [ 03/07/2020 13:07:33 ]\n",
      "2020-07-03 13:07:33,615 :: INFO :: Total Partition: 7\n",
      "2020-07-03 13:07:33,617 :: INFO :: Train: 4   Val: 1   Test: 2\n",
      "2020-07-03 13:07:33,629 :: INFO :: >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> [ CV-FOLD 1/5 ]     Training: 4    Validation: 1    Testing: 2\n",
      "2020-07-03 13:07:33,629 :: INFO :: Partition List: train [1, 2, 3, 4]   val [0]   test [5, 6]\n",
      "2020-07-03 13:07:33,629 :: INFO :: ************************ TIER 1 TRAINING - STARTED ****************************\n",
      "2020-07-03 13:07:33,634 :: INFO :: [ CAUTION ] : Resuming with pretrained model for TIER1 - ember_malconv.h5\n",
      "2020-07-03 13:07:34,282 :: INFO :: [PARTITION LEVEL TIER-1 EPOCH  : 1 ]\n",
      "2020-07-03 13:07:34,282 :: INFO :: Training on partition: 1\n",
      "2020-07-03 13:07:44.439538: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 1073741824 exceeds 10% of free system memory.\n",
      "2020-07-03 13:07:46.673028: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 1073741824 exceeds 10% of free system memory.\n",
      "2020-07-03 13:07:46.749472: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 1073741824 exceeds 10% of free system memory.\n",
      "2020-07-03 13:08:07.748322: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 1073741824 exceeds 10% of free system memory.\n",
      "2020-07-03 13:08:09.336085: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 1073741824 exceeds 10% of free system memory.\n",
      "2020-07-03 13:10:30,422 :: INFO :: Training on partition: 2\n",
      "2020-07-03 13:12:44,690 :: INFO :: Training on partition: 3\n",
      "2020-07-03 13:14:58,367 :: INFO :: Training on partition: 4\n",
      "2020-07-03 13:15:20,015 :: INFO :: Evaluating on validation data . . .\n",
      "2020-07-03 13:15:22,567 :: INFO :: Current Epoch Loss: 11.48458480834961        \tCurrent Epoch Acc: 0.4166666567325592       \tUpdating best loss: 11.48458480834961\n",
      "2020-07-03 13:15:22,567 :: INFO :: [PARTITION LEVEL TIER-1 EPOCH  : 2 ]\n",
      "2020-07-03 13:15:22,567 :: INFO :: Training on partition: 1\n",
      "2020-07-03 13:17:42,445 :: INFO :: Training on partition: 2\n",
      "2020-07-03 13:19:59,582 :: INFO :: Training on partition: 3\n",
      "2020-07-03 13:22:11,814 :: INFO :: Training on partition: 4\n",
      "2020-07-03 13:22:35,518 :: INFO :: Evaluating on validation data . . .\n",
      "2020-07-03 13:22:38,190 :: INFO :: Current Epoch Loss: 12.632720947265625\tCurrent Epoch Acc: 0.4166666567325592\n",
      "2020-07-03 13:22:38,202 :: INFO :: 1 epochs passed since best val loss of 11.48458480834961\n",
      "2020-07-03 13:22:38,213 :: INFO :: Triggering early stopping as no improvement found since last 1 epochs!  Best Loss: 11.48458480834961\n",
      "2020-07-03 13:22:39,237 :: INFO :: ************************ TIER 1 TRAINING - ENDED ****************************\n",
      "2020-07-03 13:22:39,237 :: INFO :: *** Prediction over Validation data in TIER-1 to select THD1 and Boosting Bound\n",
      "2020-07-03 13:22:44,211 :: INFO :: Selected Threshold: 100.0    TPR: 33.333\tFPR: 83.333\n",
      "2020-07-03 13:22:44,221 :: INFO :: Setting B2 boosting threshold: 0.18940125\n",
      "2020-07-03 13:22:44,232 :: INFO :: Number of files boosted to B2=0 \t[ 5 - 5 ]     Boosting Bound used: 0.18940125   Escaped FNs:0\n",
      "2020-07-03 13:22:44,385 :: INFO :: Total number of files to partition: 5\n",
      "2020-07-03 13:22:44,933 :: INFO :: Created Partition b1_val_0_p0 with 5 files and tracker csv with 5 files.\n",
      "2020-07-03 13:22:44,958 :: INFO :: *** Prediction over Training data in TIER-1 to generate B1 data for TIER-2 Training\n",
      "2020-07-03 13:23:33,023 :: INFO :: Number of files boosted to B2=243 \t[ 365 - 122 ]     Boosting Bound used: 0.18940125405788424   Escaped FNs:1\n",
      "2020-07-03 13:24:15,142 :: INFO :: Number of files boosted to B2=250 \t[ 381 - 131 ]     Boosting Bound used: 0.18940125405788424   Escaped FNs:1\n",
      "2020-07-03 13:24:53,890 :: INFO :: Number of files boosted to B2=224 \t[ 354 - 130 ]     Boosting Bound used: 0.18940125405788424   Escaped FNs:0\n",
      "2020-07-03 13:25:02,733 :: INFO :: Number of files boosted to B2=31 \t[ 53 - 22 ]     Boosting Bound used: 0.18940125405788424   Escaped FNs:0\n",
      "2020-07-03 13:25:03,190 :: INFO :: Total number of files to partition: 405\n",
      "2020-07-03 13:25:03,226 :: ERROR :: Exiting . . . Due to below error:\n",
      "Traceback (most recent call last):\n",
      "  File \"main.py\", line 65, in main\n",
      "    gtp.train_predict(model, cnst.ALL_FILE)\n",
      "  File \"D:\\03_GitWorks\\Block\\src\\core\\generate_train_predict.py\", line 115, in train_predict\n",
      "    train.init(model_idx, trn_partitions, val_partitions, fold_index)\n",
      "  File \"D:\\03_GitWorks\\Block\\src\\train\\train.py\", line 460, in init\n",
      "    b1_partition_tracker[\"b1_train\"][0] = partition_pkl_files_by_count(\"b1_train\", fold_index, train_b1data_all_df.iloc[:, 0], train_b1data_all_df.iloc[:, 1]) if cnst.PARTITION_BY_COUNT else partition_pkl_files_by_size(\"b1_train\", fold_index, train_b1data_all_df.iloc[:, 0], train_b1data_all_df.iloc[:, 1])\n",
      "  File \"D:\\03_GitWorks\\Block\\src\\analyzers\\collect_exe_files.py\", line 32, in partition_pkl_files_by_size\n",
      "    t1_src_file_size = os.stat(t1_pkl_src_path).st_size\n",
      "FileNotFoundError: [WinError 2] The system cannot find the file specified: 'D:\\\\08_Dataset\\\\Internal\\\\mar2020\\\\dummy\\\\t1\\\\4474c591edeaba256fa11f9acc9172f93a8f82f2.pkl'\n"
     ]
    }
   ],
   "source": [
    "!python main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupy",
   "language": "python",
   "name": "jupy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
